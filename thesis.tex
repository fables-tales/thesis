% The document class marks this as a thesis, supplying various options that
% control rendering of some standard features (e.g., the cover page).

\documentclass[ % the name of the author
                    author={Sam Phippen},
                % the name of the supervisor (preferably including title)
                supervisor={Dr. Rafal Bogacz},
                % the thesis    title (which cannot be blank)
                     title={Real time voice activity detectors in noisy personal computing environments},
                % the thesis subtitle (which can    be blank)
                  subtitle={},
                % the degree programme (from BSc, MEng, MSci, MSc and PhD)
                    degree={MEng},
                % the year of submission
                      year={2012} ]{thesis}

\usepackage{parskip}
\begin{document}


% =============================================================================

% This section simply introduces the structural guidelines.  It can clearly
% be deleted (or commented out) if you use the file as a template for your
% own thesis: everything following it is in the correct order to use as is.

\section*{Prelude}
\thispagestyle{empty}

A typical thesis will be structured according to a number of standard 
sections described in what follows.  However, it is hard and perhaps
even counter-productive to generalise: the goal of outlining this 
typical structure is {\em not} to be prescriptive, but simply to act 
as a guideline.  In particular, the page counts given are important 
but not absolute: their aim is simply to highlight that a clear, 
concise description is better than a rambling alternative that 
makes it hard to separate important content and facts from trivia.

You can use this document as a \LaTeX-based~\cite{latexbook1,latexbook2}
template for your own thesis by simply deleting extraneous sections (e.g., 
this one); keep in mind that the associated {\tt Makefile} could be of
use, in particular since it also executes \mbox{\BibTeX} to deal with the
bibliography.  If you opt not to do this, which is perfectly acceptible,
a standard cover and declaration of authorship produced online via
\[
\mbox{\url{http://www.cs.bris.ac.uk/Teaching/Resources/cover.html}}
\]

% =============================================================================

% This macro creates the standard UoB title page, with information drawn
% from the document class (meaning it is vital you select the correct
% degree title and so on).

\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter

% This macro creates the standard UoB thesis declaration; on the hard-copy,
% this must be signed by the author in the space indicated.

\makedecl

% LaTeX will automatically generate a table of contents, and also associated 
% lists of figures, tables and algorithms.  The former is a compulsory part
% of the thesis, but if you do not require the latter they can be suppressed
% by simply commenting out the associated macro.

\tableofcontents
\listoffigures
\listoftables
\listofalgorithms
\lstlistoflistings

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Executive Summary}

{\bf A compulsory section, of at most $1$ page} 
\vspace{1cm} 

\noindent
This section should pr\'{e}cis the project context, aims and objectives 
and main contributions and achievements; the same section may be called
an abstract elsewhere.  The goal is to ensure the reader is clear about 
what the topic is, what you have done within this topic, {\em and} what 
your view of the outcome is.

The former aspects should be guided by your specification: essentially 
this section is a (very) short version of what is typically the first 
chapter.  The latter aspects should be presented as a concise, factual 
bullet point list that summarises the main contributions and achievements.  
The points will of course differ for each project, but an example is as 
follows:

\begin{quote}
\noindent
\begin{itemize}
\item I spent $120$ hours collecting material on and learning about the 
      Java garbage-collection sub-system. 
\item I wrote a total of $5000$ lines of source code, comprising a Linux 
      device driver for a robot (in C) and a GUI (in Java) that is 
      used to control it.
\item I designed a new algorithm for computing the non-linear mapping 
      from A-space to B-space using a genetic algorithm, see page $17$.
\item I implemented a version of the algorithm proposed by Jones and 
      Smith in [6], see page $12$, corrected a mistake in it, and 
      compared the results with several alternatives.
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Supporting Technologies}

{\bf A compulsory section, of at most $1$ page}
\vspace{1cm} 

\noindent
This section should present a detailed summary, in bullet point form, of 
any third-party resources (e.g., hardware and software components) used 
during the project.  Use of such resources is perfectly acceptable: the 
goal of this section is simply to be clear about where and how they are 
used.  The content can focus on the project topic itself (rather than, 
for example, including ``I used \mbox{\LaTeX} to prepare my thesis''); 
an example is as follows:

\begin{quote}
\noindent
\begin{itemize}
\item I used the Java {\tt BigInteger} class to support my implementation 
      of RSA.
\item I used a parts of the OpenCV computer vision library to capture 
      images from a camera, and for various standard operations (e.g., 
      threshold, edge detection).
\item I used an FPGA device supplied by the Department, and altered it 
      to support an open-source UART core obtained from 
      \url{http://opencores.org/}.
\item The web-interface component of my system was implemented by 
      extending the open-source WordPress software available from
      \url{http://wordpress.org/}.
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

{\bf An optional section, of roughly $1$ or $2$ pages}
\vspace{1cm} 

\noindent
Any well written document will introduce notation and acronyms before their 
use, {\em even if} they are standard in some way: this ensures any reader 
can understand the resulting self-contained content.  

Said introduction can exist within the thesis itself, wherever that is
appropriate.  For an acronym, this is typically achieved at the first point 
of use via ``Advanced Encryption Standard (AES)'' or similar, noting the 
capitalisation of relevant letters.  However, it can be useful to include 
an additional, dedicated list at the start of the thesis; the advantage of 
doing so is that you cannot mistakenly use an acronym, for example, 
before defining it.  An example is as follows:

\begin{quote}
\noindent
\begin{tabular}{lcl}
VAD                 &:     & Voice Activity Detection                 \\
VOIP                &:     & Voice Over IP\\
%DES                 &:     & Data Encryption Standard                \\
%                    &\vdots&                                         \\
%${\mathcal H}( x )$ &:     & the Hamming weight of $x$               \\
%${\mathbb  F}_q$    &:     & a finite field with $q$ elements        \\
%$x_i$               &:     & the $i$-th bit of some bit-sequence $x$ \\
\end{tabular}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Acknowledgements}

\vspace{1cm}

This thesis would not have been possible without the continual support of my
supervisor {\bf Dr Rafal Bogacz}, who has spent many hours with me this year giving
me advice on how to organise the execution of this project, as well as ideas
for how to make improvments when I was stuck.

I would also like to thank:
\begin{itemize}

    \item My friend and mentor {\bf Dr Ben Fields}, who spent a couple of his
        afternoons discussing my thesis with me, and also for sharing his
        personal experience of writing 3 theses of his own.

    \item My good friends {\bf James Laverack} and {\bf Luke Murray} with whom I spent a
        huge amount of time unwinding when I felt burnt out.

    \item My {\bf parents}, who provided me with a much needed supply of tea during
        the long writeup of this thesis.

\end{itemize}

% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.

\mainmatter

% -----------------------------------------------------------------------------

\chapter{Contextual Background}
\label{chap:context}

\vspace{1cm}

The term voice activity detection (sometimes referred to as voice endpoint
location\cite{Tuske}) refers to the algorithms used to distinguish frames of
audio that contain speech from those that contain background
noise\cite{ramirez}. The motivation for this project is that we found, based on
personal experience, that existing VAD algorithms struggle to efficiently
distinguish the noise commonly associated with personal computers (typing and
mouse clicks) from speech.

\section{Applications}

The two main applications of VAD systems are VOIP and Speech Recognition. These
systems both deal with human voice coming in from microphone input, and are
only concerned with what the people that are actually saying, rather than the
entirety of any particular audio sequence. Given these constraints it seems
natural to place a system which only passes through the parts of the audio that
contain human speech.

\subsection{Speech Recognition}

Speech recognition systems are responsible for working out what is being said
in a given sequence of audio. They must be robust to different speakers,
environments, hesitation, stuttering and other human factors. It is often noted
that these systems degrade rapidly in the presence of noise\cite{Moreno} given
that increasingly with systems like Siri\texttrademark\cite{siri} (Apple's
speech recognition system for iOS\texttrademark) we find that people are using
systems that require accurate detection of what is being said in potentially
noisy outdoor environments.

Speech recognition systems can have their accuracy improved\cite{shin} by
employing VAD systems, which are used to locate speech endpoints. The speech
endpoints are then passed into the speech recognition system which then has to
only search the places that the VAD system has flagged up for words, rather
than the entire sequence of audio. If the VAD system is sufficiently sensitive
it may also be able to accurately locate the pauses between words the user
makes, allowing the speech recognition system to assume 1 or very few words
will lie within each part of detected speech.

There is a well known speech recognition benchmark suite called
Aurora\cite{aurora}, over this benchmark suite it was found\cite{ramirez-2}
that by improving the VAD used from the G.729 ITU-T\cite{itut} standard to a
specialised detector that word accuracy (the number of words correctly
recognized) jumped approximately 17\% in the best case.

In the context of this project we believe that our detector may be able to
improve speech recognition through improved voice activity detection, but it is
not our primary goal, as we forsee fewer circumstances where a user is going to
require speech recognition whilst interacting with their computer, than we see
cases where VOIP calls are occurring at the same time as someone is interacting
with the keyboard and mouse inputs.

\clearpage
\subsection{VOIP and Telephony}

When used in a VOIP context VAD systems are typically designed such that they
signal that frames detected as noise are not to be transmitted, preventing the
person at the other end of the call from hearing the noise. Often this is a
component in a system that also includes some noise filtering and compression
the mumble\cite{mumble} system for example includes both an amplitude and
signal-noise ratio based VAD, and then passes the output into the
CELT\cite{celt} codec, here the VAD system silences any noise detected frames
and CELT efficiently compresses the silence to ensure a bandwidth saving.

VAD algorithms allow for large bandwidth savings during both analogue
telephone calls and VOIP calls. The reason for this bandwidth saving is that in
both dialogue (two people speaking) and monologue (one person speaking) much of
the time of the call is occupied by silence. Specifically: 60\% of the time is
occupied by neither person speaking\cite{shah} in a dialogue and 20\% of the
time is occupied by neither person speaking in a monologue.

In the case of a traditional analogue telephone call, the bandwidth saving is
achieved by "Time Assigned Speech Interpolation" whereby a link can carry more
calls by assigning resources only to those calls that currently have someone
speaking on them, causing a significant saving when, for example, speech is
only travelling in one direction (1 slot instead of 2) or allocating no slots
to the call when neither party is speaking\cite{5016247}.

With a VOIP call, a significant bandwidth saving is achieved by nature of the
fact that if transmission were continuous, both parties would be transmitting
at least 64,000 bits per second\cite{ciscovad}. If we assume that for most of
the call only one person is speaking, and that our VAD is sufficiently good at
distinguishing noise from human speech then we will find at least a 50\%
bandwidth saving. Additionally if there are pauses in speech, the VAD will
provide an even greater bandwidth saving.


\section{Problem Definition}

Seeing that both user experience can be improved, and that bandwidth savings
can be achieved through the use of voice activity detection systems, the
question is what can be done to improve these. The specific goal of this
project is to build a voice activity detection system which is robust in the
typical environment of a modern computer user. Whilst these environments do
have a low background noise, they often contain very high amplitude noise
impulses that are caused by the user typing on the keyboard, or interacting
with the mouse.

As is discussed in the implementation section of this project we show that
these systems do not have a high detection accuracy when presented with these
noise impulses. Our aim is to build a VAD system that does provide a high level
of accuracy against this specific type of noise.

We believe that the problem in modern systems exist because most VAD algorithms
are designed for noisy background environments\cite{shin}, but they are not
designed for short, loud noise impulses. These noise impulses are common when a
user is typing on their keyboard. There are two modern consumer VOIP systems
that we used and were found to detect keyboard noise or mouse noise as someone
speaking. Those were: Mumble\cite{mumble}, a low latency system designed for
gamers to communicate whilst playing with each other and
Skype\texttrademark\cite{skype} a well known consumer VOIP solution designed
for single or many person VOIP calls. Especially in the case of mumble it is
essential that keyboard noise is filtered out, as gamers tend to be interacting
with their keyboard continuously whilst playing videogames, possibly drowning
out much of the real communication, or causing annoyance.

The high-level objective of this project is to build a robust voice activity
detector system that can accurately distinguish voice from keyboard and mouse
noise with suitable performance for use in real time systems. More specifically
the concrete aims are:

\begin{enumerate}
    \item Survey existing VAD systems for accuracy against our dataset
    \item Work to develop own algorithm based on machine learning techniques,
          using common features from literature
    \item Build a hangover system which smooths transition from voice to 
          non-voice classes
\end{enumerate}



% -----------------------------------------------------------------------------

\chapter{Technical Background}
\label{chap:technical}

{\bf A compulsory chapter, of roughly $10$ to $20$ pages} 
\vspace{1cm} 

\noindent

%\section {danpagematter}
%This chapter is intended to describe the technical basis on which execution
%of the project depends.  The goal is to provide a detailed explanation of
%the specific problem at hand, and any previous or related work in the area 
%(e.g., descriptions of supporting technologies, existing algorithms that 
%you use, alternative solutions proposed).  
%
%Put another way, after reading this chapter a non-expert reader should have 
%obtained enough background to understand what {\em you} have done, and then
%assess how novel, challenging and rigorous your work is.  You might view an 
%additional goal as giving the reader confidence that you are able to absorb 
%and understand research-level material.

The basic problem any voice activity detector is trying to solve is the
question of whether or not a particular frame of audio does or does not contain
speech.

In nearly all recording environments there is continuous background
noise and, when there is speech, the microphone picks up speaking on top of
that. Thusly the output of a VAD system is a single bit (1 or 0 for voice or
noise). In a VAD system we take input from the microphone and then attempt to
determine which of two input classes we are in, corresponding to whether we
should or should not transmit the audio further. We formulate this as a
decision as follows:

\begin{figure}
\[ decision = \left\{ \begin{array}{ll}
            1 & \mbox{if input like $n + v$: speech present}\\
    0 & \mbox{if input like $n$: no speech present}.\end{array} \right. \]
            \label{eqn:decision 1}
            \caption{Decision model for most voice activity detectors}
        \end{figure}

Where $n$ represents the presence of background noise and $v$ represents the
presence of conversational speech. This is referred to as additive
noise\cite{sohn} and generally represents a very realistic model of most
environments in which human speech must be differentiated from background noise.

In this project our problem space is slightly different, in that not only do we
have to deal with background noise but we also have to deal with the fact that
keyboard noise is picked up by microphones. This noise is in no way constant:
whilst most people type with a rhythm there are distinct impulses and times
when the amplitude from this noise is very low. In addition to this we may have
to deal with the case where someone is typing and talking at the same time. This
means that our decision model may look something like:

\begin{figure}
\[ decision = \left\{ \begin{array}{ll}
            1 & \mbox{if input like $n + k + v$: speech present}\\
            0 & \mbox{if input like $n + k$: no speech present}\\
    \end{array} \right. \]
            \label{eqn:decision 1}
            \caption{Decision model for our voice activity detector}
\end{figure}

Where $k$ represents the keyboard noise component of the input. This problem is
different as mentioned above, due to the highly non-static nature of keyboard
noise. It is worth noting that this model is still entirely additive, this
model is reasonable because the sources of noise should not destructively
interfere with each other. It is, however, worth noting that the phenomenon of
clipping, the decibel level of the sound sources the microphone can pick up
adding to go above it's input limit, can occur and in this case the additive
models may break down in chapter \ref{chap:evaluation} we investigate
how important these effects are.

Nearly every existing voice activity detector makes the assumption that
background noise is continuous, if irregular in volume and frequency, and
attempts to distinguish voice from the background noise. More commonly than not
we have found that these detectors merely identify any sound that is not part
of the continuous background noise, giving real scope to our project.
Specifically identifying sudden impulses of sound from the background, and
determining whether or not they are someone speaking or someone typing is the
main aim of our system.

\section{Understanding voice}

In particular there are two types of speech, voiced and unvoiced speech. The
easiest way to demonstrate the difference between this is to place two fingers
on your throat/lower chin and then make a "sssssssss" sound (unvoiced) followed
by a "zzzzzzzzzzzz" sound (voiced). In the first example one cannot feel any
vibration, however in the second example one feels ones throat vibrating.
Voiced and unvoiced speech have somewhat different characteristics and this
must be taking account of when building a VAD system.

When speaking voice typically has it's fundamental frequency in the 75-150HZ
range for men and the 150-300HZ range for women. However, the human voice when
speaking also has a rich harmonic content meaning that there are a number of
other frequency peaks. The voice band, the band used for transmission of speech
lies in the 300HZ to 3400HZ range. Whilst this may seem that it would cut off
the sound of voice. This is not the case as enough of the harmonics of speech
still make it through, and the listener will get the impression that they are
actually hearing the missing fundamental tone.

The harmonics of the fundamental are integer multiples in frequency of the
original note, demonstrated by figure \ref{img:harmonics}, we see that the
first harmonic is the wave with a base frequency, the second is with twice that
frequency and so on. Due to the complex physics of how our bodies force air through
our throats we get a large frequency response not only at the fundamental notes
of voice, but also at the first few harmonics of the fundamental.

We can see that a section of voice features strong harmonics of the fundamental
in figure \ref{img:spectrogram} a spectrogram, showing the frequency strength
over time for a segment of speech. Specifically with the first word note the
strong fundamental at 600HZ with strong harmonics at 1800HZ (3rd harmonic) and
2400HZ (4th harmonic).

\begin{figure}
    \includegraphics[height=10cm]{harmonics.png}
    \label{img:harmonics}
\end{figure}

\begin{figure}
    \includegraphics[height=10cm]{voder_spectrogram.jpg}
    \label{img:spectrogram}
\end{figure}

\section{Existing Voice Activity Detection Techniques}

Existing voice activity detection techniques fall into one of two categories.
Either the algorithms contain a number of parameters, and each parameter is
given a threshold (the noise/voice threshold). A decision is made based on the
thresholds over all the parameters as to whether transmission should or should
not occur. In some systems the thresholds are static with the thresholds
determined experimentally over some training data\cite{haigh}. These systems,
by their design have trouble dealing with background noise that changes such as
computer fans spinning up/down, or a person using a mobile phone walking through
streets with different activities happening in the background

In other threshold based voice activity detection systems the thresholds are
adaptive\cite{gokhun}. These systems are designed to deal with non-stationary
background noise, that is background noise that changes in amplitude and pitch.
These systems usually start with an initial guess of their class thresholds
which then change over time. The thresholds change based on the running average
of each of the parameter values for the two classes, as output by the
detector\cite{sakhnov}

The other major class of voice activity detectors apply machine learning
algorithms to build a complex classification boundary over the parameters of
the model\cite{shin}. These classifiers do suffer from the problem of having a static
decision boundary, due to the fact that on-line training is not possible
without hand labelling. This, however, usually does not cause too much of an
issue due to the fact that the more complex decision boundary achieves a much
higher accuracy when trying to decide between the voice and non-voice class.

\subsection {Features}

In both threshold based and machine learning based VAD systems a number of
features are extracted from the raw audio in order to give it a representation
that is amenable to distinguishing frames of audio that contain background
noise from those that contain speech. In this section we investigate existing
features that are used in models for voice activity detection, including those
that were and were not used in our model.

\subsubsection{Commonly used features}

When building our own classification system it is important to find the
features that are already being used to build VAD systems, such that they can
be used in our system, in combination with the classifier discussed in section
TODO ADD A REFERENCE SRSLY. Whilst our problem is slightly different in that we
have extremely non-stationary background noise, these features may be useful to
build an accurate VAD system. This also serves as the explanation for when
these features are referred to later on in this paper.

One of the most commonly used features in voice activity detection is the
energy of a frame. Given in figure \ref{eqn:energy} (sometimes also implemented
as the root mean square of a window), this feature is used in
\cite{shin}\cite{sakhnov}\cite{gokhun}\cite{haigh}\cite{atal} and \cite{sohn2}.
The energy feature of a particular window of audio has been found to be
significantly higher in frames that contain speech than background
noise\cite{atal}. However, this parameter also forms different distributions
for voice and unvoiced speech, with the energy value for voiced speech being
significantly higher than that for unvoiced speech. In \cite{atal} they state "
The energy of un-voiced sounds is usually lower than for voiced sounds, but
often higher than for silence." The system they used was a threshold based one,
however in \cite{shin} this feature was used as part of a decision tree system.
They reported "The full band energy, the conventional feature for endpoint
detection for clean speech, does not work well for noisy speech." The feature
did, however, give roughly 88\% accuracy figure by itself when used in their
classifier. This feature was used in our system for classification.

\begin{figure}
    $$Energy(x) = \sum_{i=0}^{length(x)}x(i)^2$$
    \label{eqn:energy}
    \caption{Energy of a window x, where length(x) gives the number of individual
    samples in x}
\end{figure}

Another commonly used feature in voice activity detection is the zero crossing
rate of a window. The definition for this feature is given in figure
\ref{eqn:zero-crossing-rate}. This feature is not used in \cite{shin}, but is
used in \cite{atal}. In \cite{atal} it is stated that zero crossing rate varies
consistently with energy. This is very important for a classifier based system
like the one that we are building because features that correlate with each
other weakly are likely to give different statistical "views" on the data that
machine learning algorithms can extract much of variance from in order to give
a more accurate classification. In \cite{shin} they state that zero-crossing
does not perform well in noisy speech, they did not use this parameter at all
and did not make any actual reports on the accuracy of the feature. In
\cite{haigh} they found that a detector based on energy and zero-crossing rate
only failed in noisy conditions. This feature was also used in our detector.

\begin{figure} $$ZeroCrossingRate(x) =
    \sum_{i=1}^{length(x)}abs(sign(x(i-1))-sign(x(i)))$$ \label{eqn:zero-crossing-rate}
    \caption{Energy of a window x, where length(x) gives the number of
    individual samples in x, abs is the absolute value of a number, and sign
gives the sign of the number 1, 0, or -1 for positive, zero or negative numbers
respectively} \end{figure}

\subsubsection{Other features from existing systems}

There are anumber of other features that were used in the system we built that
came from existing implementations of voice activity detection. We here explain
these features so that the reader is aware of how the feature values are
calculated.

Another feature that is used in \cite{shin} is referred to as a "band energy".
This simply takes the energy as specified in \ref{eqn:energy} but instead of
computing it over the entire window, it is taken over the amplitude component
of a fourier transform that has had a band pass applied to it, with the
frequency band specified. For example the 300-3700HZ band energy that Shin et
al refer to (which corresponds to the audible range of a telephone system) is
computed by taking the sum of squared amplitudes of the components of the
amplitude domain of a fourier transform that lie within the 300-3700HZ
frequency range. In our system we used multiple band energies as discussed in
chapter \ref{chap:execution}.

In \cite{haigh} they used ceptral features for voice activity detection. There
are multiple transforms that are referred to as the "cepstrum" of a signal,
specifically the complex cepstrum, the power cepstrum and the phase
cepstrum\cite{childers}. The power cepstrum is defined in
\ref{eqn:power-cepstrum}. This feature is primarily used in speech recongition,
the higher order task of determining what words are being said, rather than
voice detection\cite{muda} and in \cite{atal} they criticize this approach for
VAD, stating that this feature requires a very high degree of periodicity
(singal approximately repeats itself with a fixed period) to provided an
accurate classification.

\begin{figure}
    $PowerCepstrum(f)=\mathcal{F}^{-1}\{\mbox{log}(\mathcal{F}\{ f(t) \}|^2)\}|^2$
    \caption{Definition of a power cepstrum. $f$ is our
    signal function, $\mathcal{F}$ is the fourier transform function. $|a|$}
    \label{eqn:power-cepstrum}

\end{figure}

Specifically in \cite{haigh} they use a cepstral distance measure defined as
the euclidian distance of the power cepstrums between the data they were
attempting to classify and a pre-defined 'code-book' of samples, selecting the
class of whichever example in the 'code-book' had the lowest euclidian
distance. This is equivalent to a single nearest neighbor classification. An
accuracy figure is not given however they were able to design the threshold of
their system such that it tripped at a very low voice likelyhood ratio (0.01) with
relatively low error.

In \cite{moattar} one of the key features that they extracted was the dominant
frequency component of the signal. This corresponds to the highest peak in the
fourier transform of the signal and is defined in \ref{eqn:dom-freq}. The
authors of the paper observed that this feature was helpful in discriminating
speech and silence frames. We observed problems with this feature due to the
nature of our windowing system which are discussed in \ref{chap:execution}. The
accuracy of this feature is reported in \cite{moattar} to have a greater than
80\% accuracy under even noisy conditions. However the caveat to this feature
for our project does apply that our noise is highly irregular and most
definitely not of stationary amplitude.

\begin{figure}
    $DominantComponent(f) = argmax_i(|\mathcal{F}(f(t))|)$
    \caption{Definition of the dominant component of a signal. $f$ is the singal
        function, $\mathcal{F}$ is the fourier transform. $argmax$ gives us the
        input corresponding to the maximum output of a function.}
    \label{eqn:dom-freq}
\end{figure}

In \cite{atal} they used two features constructed via a linear predictive
coding (LPC) analysis. The basic principle of linear predictive coding is that
the current sample can be approximated as a linear combination of the previous
samples\cite{rabiner}, the coding is the coefficients ($a_1,a_2,...,a_p$) that
each of the previous samples are combined with to give the prediction of the
coding. In \cite{atal} they specifically used a size 12 linear combination,
which means that each sample was predicted as a linear combination of the
previous twelve. It is important to note here that over short periods of
10-30ms voice signals are roughly periodic, and that it is obvious that $s(n)
\approx s(n-N_p)$ where $N_p$ is the length of the period. LPC techniques work
with window sizes that are significantly smaller than the period and still
accurately reconstruct the next part of the signal. The result of the LPC is a
number of coefficients that can be used to attempt to reconstruct parts of the
signal, however, they can also be used as parameters to both threshold and
classifier based systems. In \cite{atal} they used two parameters from the LPC:

\begin{itemize}

    \item The first coefficient of a 12 coefficient LPC analysis: this is the
        primary coefficient $a_1$ of the LPC analysis, corresponding to the
        sample nearest to the sample we are trying to predict.

    \item The energy of the error in the prediction made by the LPC. This is
        essentially defined as the error signal (computed by subtracting the
        signal predicted by the LPC and the actual signal). Energy is defined
        above as the sum of the squares of the signal in \ref{eqn:energy}. In
        the paper they state that this feature is equivalent to the ratio of
        the geometric mean of the spectrum to the arithmetic mean of the
        spectrum.  This makes it equivalent to the spectral flatness measure
        defined in \cite{moattar}

\end{itemize}

\section{Machine learning techniques}

In this project we decided to use machine learning techniques to build our VAD
system. This is primarily due to the fact that threshold based approaches
report lower accuracy rates in the literature and are very much designed to
differentiate continuous background noise from sudden changes in any of their
parameters. Our feeling was that this would not be amenable to differentiating
sudden keyboard impulses from sudden voice impulses. In the execution chapter
of this project we discuss how we built the training set and how we built the
training set for this system, and this section specifically conducts a review
of machine learning systems, with an explanation of state of the art machine
learning techniques such as random forests, gradient boosted regression trees
and adaboost with decision stumps. All classifiers here are discussed under the
assumption that they only have to deal with a binary classification problem,
which corresponds to the classification problem that we are dealing with
(voice/non-voice classification).

\begin{figure}
    $Train:R^x \times R^y, \{0,1\}^y \rightarrow ModelFunction$

    \caption{Definition of a training function: x is the number of features and
        y is the number of classes. The function takes an x by y matrix of
        reals and a y-length vector of labels and ouptuts a model function that
        behaves as a classification function defined in in
    \ref{eqn:define-classify} }

    \label{eqn:define-train}
\end{figure}

\begin{figure}
    $Classify: R^x \rightarrow \{0,1\}$
    \caption{Definition of a trained model/classification function: x is the
        number of features, the model takes a feature vector and outputs a class,
    in some implementations this may be a probability}
    \label{eqn:define-classify}
\end{figure}

\subsection{Classification and regression trees}

Classification and regression trees are class of machine learning algorithm
that work by first training a binary decision tree and then pushing examples to
be classified through the binary decision tree to perform
classification/regression. Training is performed by computing splits in the
feature space that represent the maximum gain in information over the entire
set of features for the examples that will reach this part of the tree. At the
start the split that gives highest information gain over the entire data set is
chosen, giving us two subsets of the entire data set, that are then split
seperately from each other and so on and so on. This process continues until
the leaf nodes of the tree contain only training examples of the same class. By
this method we ensure that all training examples will be classified correctly,
in general this method can lead to some overfitting that will cause the classifier
to not generalise as well, and the trees are pruned. An example decision tree
is shown in Figure \ref{fig:decision-tree}.

\begin{figure}

    \includegraphics[height=10cm]{decision_tree.png}

    \label{fig:decision-tree}
    \caption{An example decision tree kindly provided by Wikipedia}
\end{figure}

Regression trees are used when the output is not a single class but instead
some single real number. For example a regression tree might be used in an
attempt to predict the length of a patients stay in hospital. In \cite{shin}
individual features were passed through a speech/non-speech classification, and
then a CART (classification and regression tree) was employed over the outputs
of the pre-classifiers to come to the final decision. This approach gave a very
high accuracy level under low and medium noise levels (-5 and 0db respectively)
but degraded under high noise levels (5db). In order to improve the
classification parmeters were passed in from more than one window (specifically
the previous and next window were passed in before the decision was made).

\subsection{Decision Stump}

Whilst never used on its own the decision stump classifier is worth exploring
due to it's use in ensemble\footnote{Ensemble classification methods are a
    large class of classification algorithm that combine many classifiers in an
    attempt to improve classification accuracy or generalizability or both. In
chapter \ref{chap:evaluation} we show that these methods give much better
accuracy in cross validation than non-ensemble techniques over our data set
with some intuition for why this is the case.} classification methods.
Specifically the decision stump classifier acts like a normal binary decision
tree classifier or CART but instead of training until completion it is trained
for only one split. These often acheive very low classifcation accuracy
(usually just barely better than a random classifier) and are sometimes
referred to as "weak" or "base" classifiers. In the famous Viola Jones face
detector\cite{viola} many decision stumps were used to form a strong classifier
that achieves very good classification accuracy. An example decision stump is
shown in Figure \ref{fig:decision-stump}.

\begin{figure}

    \includegraphics[width=5cm]{decision_stump.png}

    \label{fig:decision-stump}
    \caption{An example decision stump kindly provided by Wikipedia}
\end{figure}

\subsection{Adaboost}

Adaboost is a well known algorithm for ensemble classification that achieves
a high classification accuracy by training many classifiers. Unlike some of
the other ensemble methods we discuss in this paper the Adaboost method can
be used with any base classifier, and as such may be amenable to a larger set
of problems than other ensemble methods presented here. Specifically the result
of the Adaboost training is a chain of classifiers which become increasingly
biased towards the most difficult samples to accurately classify within the
training set.

The final classification output of the Adaboost algorithm is given as a linear
combination of weights multiplied by the classification output (usually a
posterior probability) of each of the individual classifiers within the Adaboost
classifier. The pseudocode for training the system is given in \ref{pseudo:adaboost}.

Within this paper we use an Adaboost algorithm that uses decision stumps as its
base classifier, this decision was made based on the fact that we wanted
training to complete quickly (decision stumps train in effectively no time) and
that this approach has been shown to work well in other problem
domains\cite{viola}.

It is worth noting that training an Adaboost classifier with a very large
number of weak classifiers can cause a high level of overfitting, especially
in the presence of noise within the data set.

\begin{figure}
    Psuedocode for training an adaboost classification system. It is important
    to note that the weak classifiers must be sensitive to the weights given in
    the $w$ array. If the classification system that is used is not sensitive
    to these weights (for example a decision tree by default has no concept of
    weighting it's inputs) we can weight the samples by performing a roulette
    wheel selection\footnotemark on the samples.

    \vspace{3em}

    \begin{enumerate}

        \item let $w_1(i)=\frac{1}{N_{samples}}$ for all $i$ in
            $0,1,...,N_{samples}$.

        \item for i = 0 to number of classifiers in ensemble (n)
            \begin{enumerate}

                \item Train a weak classifier that returns -1 or 1 given a
                    sample.
                \item Choose $\alpha_i \in R$.
                \item Update $w(i) = w(i)exp(-\alpha y_i h_t(x_i))$ for all $i$ in
            $0,1,...,N_{samples}$.
            \item Normalize $w(i)$ so that it sums to 1 (forms a probability distribution).

            \end{enumerate}
        \item output strong classifier: $$H(x) = sign\left(\sum_{i=0}^{n}\alpha_i h_i(x)\right)$$
    \end{enumerate}

    \vspace{3em}

    In step 2a in the previous listing we specifically pick the weak classifier
    that gives the lowest weighted prediction error over all the samples, where
    the weighted prediction error is defined as: $$error_{h_i} =
    \sum_{k=0}^{N_{samples}} w(k)(y_k\neq h_i(x_k))$$. By doing this at each
    stage we are effectively picking the weak classifier that is most capable
    of dealing with the most highly weighted samples. This gives us an improved
    classification over a single classifier because the difficult samples are
    given "specialist" classifiers that are more capable of dealing with them
    than a classifier trained over the entire set.

    In step 2b we specifically chose $\alpha_i$ to greedily minimize
    classification error at each step, that is at every step the overall
    classification error on the training set will go down rather than up.

    \label{pseudo:adaboost}
    \caption{Adaboost training psuedocode and explanation}

\end{figure}
\footnotetext{Roulette wheel selection is an algorithm which
        allows us to select randomly from a weighted array. If the array forms
        a probability distribution (that is that it sums to 1) then we can
        generate a number uniformly at random and move through the array of
        weights, subtracting each weight from the generated number, when the
        number reaches zero we have arrived at the item that corresponds to the
    randomly generated number, and we can output it. When performing this
selection multiple times we end up with a distribution of selections from the
array that matches the weights of the items.}

\subsection{Random Forest}
\label{section:random-forest}

The random forest classification algorithm is a modern ensemble method for
classification proposed by Breiman in \cite{breiman}. In a random forest many
decision trees are combined to give a much better classification result than
any single classifier is able to. In order to arrive at a final class the
original Breiman paper suggested that each classifier's output is taken and the
modal class is used. In the implementation that we use in this paper we instead
ask each tree to give a probability value that the instance is in the positive
(voice) class and then take the mean average of these, thresholding at 0.5 to
give the voiced/unvoiced classification. This helps to more accurately reflect
that some trees may be very confident with an instance and give a high
posterior probability but some may be less confident and give a probability
closer to 0.5, the mean of these allows the end result of the classification to
be more sensitive to the results of the individual trees.

The training method for a randomised forest as described in \cite{breiman} is
to create a randomised vector from the training set by performing selection
without replacement at random from the training set, such that any single
instance may occur more than once, this is the same technique that is used in
the bagging ensemble classification method. The difference between how this
system is trained, and how bagging trains is that instead of then just training
decision trees in the traditional way, the random forest approach builds splits
in each tree by selecting random subsets of the possible features at each of
the splits.

There is a second class of random forest used in this paper called extremely
randomised trees. This method goes one step further in the randomness during
training and instead of splitting on the most discriminative threshold of the
most discriminative feature of a random subset of the features at each training
stage the extremely randomised forest picks random thresholds of each feature
in the random subset of features and then picks the one with the highest information
gain.

In \ref{fig:comparison} we can see a comparison of decision trees, random forests,
extremely randomised trees and Adaboost. It is worth noting that the decision
boundaries for random forests are a lot noisier around noisy data than the
other classification algorithms.

\begin{figure} \label{fig:comparison}
    \includegraphics[width=13cm]{classifier_comparison.png} \caption{Comparison
        of decision trees and some ensemble classification techniques, showing
        data points with circles and decision boundaries with background color,
        shading in the background colour indicates probability where the
        ensemble methods have internal classifiers that disagree. The decision
        tree always has block color as it gives a single class as an output
rather than a probability} \end{figure}

\subsection{Gradient Boosted Regression Trees}

The gradient boosted regression trees algorithm uses boosting to build a strong
classifier by combining many decision trees over the input training samples.
Like Adaboost the GBRT (Gradient Boosted Regression Trees) model takes the
output of the overall classifier as the sum of each of the outputs of the
individual classifiers in the ensemble multiplied by a weight. The difference
between this technique and Adaboost is that instead of weighting samples that
are classified incorrectly, each internal classifier is trained with the
individual examples labelled with the residual between the actual label and
what the current version of the classifier gives.

The main disadvantage of GBRT in comparison to the Random Forest technique is
that the training process cannot be easily parallelized, due to the fact that
each new tree must be trained as a function of the ensemble classifier as it
has been trained thus far.

In \cite{parker} they use GBRT to perform sequence alignment of music, this
technique was also used by the winning submission to the netflix prize.

\subsection{A review of ensemble techniques}

The ensemble techniques that we have discussed sofar all have been shown to
perform significantly better in generic machine learning applications than
single classifiers this is because they build a number of different models in
various ways, each of which has either a randomised component, or is reliant on
moving against the error of other models within the ensemble. These techniques,
do, however come with a high performance overhead, due to the fact that there
are many classifiers instead of one. Adaboost with decision stumps does
somewhat serve as an exception to this rule because decision stumps take
virtually no time to perform a classification. In this paper we consider
decision trees, random forests, gradient boosted regression trees and Adaboost,
comparing the feature set that we developed in an attempt to maximise accuracy
and generalizability.

% -----------------------------------------------------------------------------

\chapter{Project Execution}
\label{chap:execution}

{\bf A topic-specific chapter, of roughly $20$ pages} 
\vspace{1cm} 

%\noindent
%This chapter is intended to describe what you did: the goal is to explain
%the main activity or activities, of any type, which constituted your work 
%during the project.  The content is highly topic-specific, but for many 
%projects it will make sense to split the chapter into two sections: one 
%will discuss the design of something (e.g., some hardware or an algorithm), 
%inc. any rationale or decisions made, and the other will discuss how this 
%design was realised via some form of implementation.  
%
%This is, of course, far from ideal for {\em many} project topics.  Some
%situations which clearly require a different approach include:
%
%\begin{itemize}
%\item In a project where asymptotic analysis of some algorithm is the goal,
%      there is no real ``design and implementation'' in a traditional sense
%      even though the activity of analysis is clearly within the remit of
%      this chapter.
%\item In a project where analysis of some results is as major, or a more
%      major goal than the implementation that produced them, it might be
%      sensible to merge this chapter with the next one: the main activity 
%      is such that discussion of the results cannot be viewed separately.
%\end{itemize}
%
%\noindent
%Note that evidence of ``best practice'' project management (e.g., use of 
%version control, choice of programming language and  so on) should only 
%be included if there is a clear reason to do so.

\section{Building the dataset}

One of the most important things for the classification based approach that we
have decided to use in this project is to build a dataset that is
representative of the problem domain, and to ensure that the dataset has a
representation that is amenable to classification. In this section we will
discuss our approach to constructing a dataset that allows us to achieve the
aims outlined in this paper.

The first step in building the dataset was to record a reasonable amount of
audio that can be used to build the classifier. In order to build our sample
database we used the recording feature of the Mumble\cite{mumble} program,
connected to the server for the Computer Gaming Society of the University of
Bristol Students union. Due to the fact that there were multiple players during
each session, we were able to capture separate recording channels for each
player in the game. In total this database amounts to 24 hours of audio,
although not all of this audio was used in the overall classification system.
The audio captured was of extremely high quality at 48KHz with 24 bit samples.
This was compressed down to 16 bit samples at 48KHz for use in this project due
to the large memory requirements of pulling the entire set into memory. 

It is worth noting at this point that while there are existing datasets such as
the Buckeye corpus\cite{buckeye}, none of them contain noise that comes from
keyboard and mouse inputs, which is the critical target of our project. It is
also worth noting that due to the large number of people that were involved
with the recording we get a large number of different microphone/keyboard/mouse
configurations to build a classifier that is robust to these different
configurations. Additionally it is worth noting that there is a minor gender
imbalance in this dataset, with 10 males and only 1 female in the data set.
Whilst this may cause the classifier to generalise less well, the data set was
recorded from the best available source at the time.

Once the audio had been recorded the task of labelling presented itself.
Initially this labelling was performed by opening the audio in the audio editor
Audacity\cite{audacity} and then selecting regions of audio and playing them
back, this approach had the advantage of allowing relatively large, continuous
regions to be labelled quickly, but it was a very slow process when there were
short regions where people would utter short words and then return to the
silence/typing class. In order to speed this process up a simple python script
was written with a very simple class detector. In order to make a best guess
the script ran on 200ms windows of audio, detecting whether the sum of the
absolute value of each of the frames was above some very low threshold, if the
sum of the current frame was the same side of the threshold as the previous
frame then they were joined together, this meant that the smallest audio frame
that the user had to label was 200ms, and the longest would be when there was
contiguous speech, which based on labelling experience was around 4 seconds. An
example waveform with a contiguous region of 2.6 seconds, followed by a 200ms
region of shortest change is given in figure \ref{img:labelling_example}. This
allowed for background noise to be separated from both sharp keyboard impulses
and people speaking, it then interactively asked the user to label the audio by
hand. In addition to this windows of total silence were simply given a
homogeneous background noise label (due to the fact that frames with 0
amplitude clearly do not represent someone speaking). The labelled data was
stored in an sqlite database, with the start time, end time, original file name
and the class label. It is worth noting that whilst this approach allowed for
quick labelling there were times where the same class was given to a large
range of audio which was primarily in one class, with a very short tail of the
other class at the end. We feel that this problem did not skew the
classification results too much, due to the very high classification accuracy
that we were able to build.

It is worth noting that due to the fact that we stored the data and the class
information separately we are able to apply transformations to the raw audio
data without affecting the accuracy of the labelling. This was useful when we
started performing experiments such as changes in bits per sample and frame
rate which have some important effect on classification accuracy. It is also
worth noting that we did not end up labelling all 24 hours of audio that was
available: this would taken an incomprehensibly large amount of time and we
felt that we could still build a valid proof of concept classification system
with the data that was available to us, specifically we have just over an hour
of labelled audio, split amongst all but one of our speakers (one of the
males). The distribution of labelled data amongst the various participants can
be seen in figure \ref{img:speaker_distribution}. The dominant speaker, speaker
7 was taken from the recorder's configuration, the most data is available
because the recorder was always present in any of the recording sessions,
whereas other users were not. Speaker 3 is the female speaker and forms a
representative part of the dataset.

\begin{figure}
    \includegraphics[height=10cm]{speaker.png}
    \label{img:speaker_distribution}
    \caption{Distribution of speakers in the labelled dataset}
\end{figure}

\begin{figure}
    \includegraphics[width=8cm]{labelling1.png}
    \includegraphics[width=8cm]{labelling2.png}
    \label{img:labelling_example}
    \caption{Example of contiguous and short region in simple labelling system}
\end{figure}

\section{Building the classifier}

From the start we used the Random Forest classifier discussed in section
\ref{section:random-forest}, this is due to both familiarity, and availability.
Specifically we took an off the shelf implementation from the
scikit-learn\cite{sklearn} Python library. This library was chosen due to it's
general popularity and the author's familiarity with python. Once the
classifier had been chosen we decided to act on windows of 16ms, this number
was chosen because it is used in gaming applications as the target framerate of
any on screen graphics, corresponding to 60 frames per second, which most
players perceive to be acting in real time. The background research indicates
that this frame size is not outside the desired range of 10-30ms, it is worth
however noting that the Shin\cite{shin} system uses a slightly shorter, 10ms
window size.

In this section, classification accuracies are reported as the result of a 10
fold, randomised, stratified cross validation (there was a minor imbalance in
the number of samples in each of the classes, with roughly 53\% background
noise and 47\% voice). The folds were trained with 66\% of the data in the
training set, we felt that this split gives a reasonable tradeoff between
giving the classifier sufficient data to train on, and not training in such a
way that the classifier would be overly biased towards the dataset, in other
words we are trying to retain some of the generalizability of our classifier by
using a 66\% training set rather than a 90\% training set.

The general binary classification problem that we formulated in the technical
background chapter of this paper requires that we have some fixed size vector
of real numbers for each sample in the training/test sets, obviously this means
that we cannot pass some arbitrary length audio stream into the classifier and
attempt to build a result, so the natural thing to do is to apply a "windowing"
to the audio. Specifically: we must pick a fixed window size that is short
enough that it will not be perceived as lagging whilst classification is
occurring, short enough that we can classify it in real time (if our window
contains 1 million samples it is unlikely that we can classify it in real
time), and long enough that it contains enough information so that it can be
classified.

We initially chose a 16ms window size as the size of the window for
classification, this falls within the common window sizes in the background
literature, which typically fall in the 10ms-30ms, and our specific choice of
16ms was made because this is the traditional framerate that many gaming
applications target, representing 60 frames a second, traditionally perceived
as smooth by most gamers. In chapter \ref{chap:evaluation} we assess the
relative merits of window sizes in the 10-30ms range, and the affects those
have on classification accuracy. Our initial choice of 16ms, with the 48000Hz
audio that we use gives us 768 samples per window. Initially we decided to pass
all 768 samples as a vector as the representation of the window for
classification.

The classifier with 768 samples gave a relatively unsatisfying classification
accuracy (blah). It was obvious that the raw samples were not amenable to
classification due to the position of individual windows having very little
with the actual audio that was happening, and more importantly each frame
within the 16ms window basically took every value regardless of class, so we
began to implement features from the background research to begin to improve
classification accuracy.

In order to allow easy construction of a representation of the audio that would
allow for a high classification accuracy, we added another stage to the
pipeline that sat between the audio on the hard drive and the classifier.
Specifically the pipeline had these stages:

\begin{itemize}

    \item For reach row of the database, extract class label, start time, end time
        and file name

    \item For each of those, open the file for reading and read samples out of
        the file between the start time and the end time

    \item Cut the sample into 16ms windows, appending 0s if necessary.

    \item Apply all feature transforms to get a uniform width vector of integers

    \item Send feature vectors along with class labels into the classifier for
        training.

\end{itemize}

This allowed us to mix feature transformations as the project went on, removing
and adding as necessary. The features that were implemented were:

\begin{itemize}
    \item Energy: the sum of squares of the frame, referred to in the technical
        background chapter.

    \item Zero crossing rate: the number of times the samples in the frame
        change in sign, referred to in the technical background chapter.

    \item The energy of the 3500 to 4300Hz frequency band: implemented as a
        squared sum of the fourier magnitude domain of the values in this
        frequency band. This feature was implemented due to it corresponding to
        a peak in the average fourier transform of one of the classes

    \item The energy of the 5400 to 6800Hz frequency band: implemented as above,
        This feature was implemented due to it corresponding to a peak in the
        average fourier transform of one of the classes.

    \item The energy of the 1800 to 2700Hz frequency band: this corresponds to
        the most dominant frequencies of voice according to the research

\end{itemize}

These frequency bands are shown along with the complete fourier amplitude domain
in \ref{fig:frequencies}

\begin{figure}
    \label{fig:frequencies}
    \includegraphics[height=10cm]{frequencies.png}
    \caption{Histogram of fourier domain for voice and noise classes, voice
    at the top, and noise at the bottom, with the frequency bands highlighted}
\end{figure}

\chapter{Critical Evaluation}
\label{chap:evaluation}

{\bf A topic-specific chapter, of roughly $10$ pages} 
\vspace{1cm} 

In this chapter we have given the results of a number of experiments that were
performed over our classifier built in the previous chapter, with regards to
various facets such as classification accuracy, performance and
generalizability.

\section{Sample Bits}

In the technical background chapter we explained how each sample of audio is
represented by a number of bits, more bits giving much higher fidelity audio
recreation. We started with 16 bit audio samples, and in this experiment we
investigate the affect of changing to 8 bits per sample instead. There are a
number of compelling reasons to make this change:

\begin{enumerate}

    \item If the bits per sample can be reduced, the overall data rate going
        through the system will be reduced, this may give performance benefits
        such as being able to apply the feature transformations more quickly,
        and fitting more data in the cache lines. Particularly as VAD systems
        are used in embedded/mobile computing contexts, being able to work with
        smaller data sets is important

    \item Additionally it means that the data set that we use can be more
        aggressively compressed for serving on the internet: we have built a
        unique corpus that includes audio like laughter and keyboard noise
        which were not found in the existing literature, and as such distributing
        the data set more easily is a benefit

\end{enumerate}

We obviously expect to see a reduced classification accuracy from cutting out
the lower 8 bits of audio information, but it is worth noting that unlike
music, voice tends to have a fairly uniform amplitude, and so the minor
variations may not have that much of an effect.

To perform this experiment, we modified the part of the program that reads the
audio from the files, and instead of just passing out the entire audio sample
we masked out the lower 8 bits of the 16 bit sample, to get an initial
indication of how much of an affect removing these bits would have on the
accuracy of the classification system. In order to perform testing, we trained
our classifier system on 50\% of the data in the training set, testing against
the over 50\% and plotted the roc curves which are shown in \ref{fig:roc-bitcrush}

\begin{figure}
    \includegraphics[height=10cm]{roc_8bit.png}
    \caption{ROC curves of the two classifiers}
    \label{fig:roc-bitcrush}
\end{figure}

This curve clearly shows that the accuracy of the two classifiers is nearly the
same (98\% area under the ROC curve for the full bit samples, versus 97\% area
under the ROC curve for 8 bit samples). This finding is useful, it means that
the classifier can be trained on smaller data sets, reducing the overall size
of the classification model that has to be loaded into memory: the full
classifier takes 65 megabytes of memory, the reduced one takes only 40. This
means that, for example, this system could be released on both PC and embedded
contexts, with the slightly higher accuracy version being released on PC and
the lower accuracy version released on the embedded/mobile devices. One of the
main aims of this project is to release our corpus after we have completed, and
it is important to note that reducing the size of the archive that we release
will directly increase the number of people that it is available to. By halfing
the bitrate (by reducing the number of bits per frame) we think this result is
genuinely a good way to improve our ability to release the system, and improve
performance.

\section{Frame Rate}

Changing the frame rate will naturally be expected to alter classifier accuracy
and performance: firstly having to work with fewer frames will mean that the
fourier transforms that we use are more sparse, and that we have to work over
less data when computing features like the energy of the frame. This may also
lead to a reduced classification accuracy. There is some reason to believe that
this may not have as much impact as changing the bit rate of the classifier,
this is due to the fact that human voice rarely goes above 4000Hz, with a
nyquist limit of 8000Hz. Our audio samples being recoreded at 48000Hz means
that we may have vastly much more information than we need. In order to test
this we initially started by taking half (24000Hz) and quarter (12000Hz) frame
rates, to determine whether or not this would have a significant affect on
classification accuracy. The results are shown in \ref{fig:roc-frameratecrush}.

%\noindent
%This chapter is intended to evaluate what you did.  The content is highly 
%topic-specific, but for many projects will have flavours of the following:
%
%\begin{enumerate}
%\item functional testing, inc. analysis of failure cases,
%\item performance results, and analysis of said results that draw some 
%      form of conclusion,
%      and
%\item evaluation of options and decisions within the project, and/or a
%      comparison with alternatives.
%\end{enumerate}
%
%\noindent
%This chapter often acts to differentiate project quality: even if the work
%completed is of a high technical quality, critical yet objective evaluation 
%and comparison of the outcomes is crucial.  In essence, the reader wants to
%learn something, so the worst examples amount to simple statements of fact 
%(e.g., ``graph X shows the result is Y''); the best examples are analytical 
%and exploratory (e.g., ``graph X shows the result is Y, which means Z; this 
%contradicts [1], which may be because I use a different assumption'').  As 
%such, both positive {\em and} negative outcomes are valid {\em if} presented 
%in a suitable manner.

% -----------------------------------------------------------------------------

\chapter{Conclusion}
\label{chap:conclusion}

{\bf A compulsory chapter, of roughly $2$ pages} 
\vspace{1cm} 

\noindent
The concluding chapter of a thesis is often underutilised, in part because
it is often left until close to the deadline and hence does not get enough 
attention.  Ideally, the chapter will consist of three parts:

\begin{enumerate}
\item (Re)summarise the main contributions and achievements, in essence
      summing up the content.
\item Clearly state the current project status (e.g., ``X is working, Y 
      is not'') and evaluate what has been achieved with respect to the 
      initial aims and objectives (e.g., ``I completed aim X outlined 
      previously, the evidence for this is within Chapter Y'').  There 
      is no problem including aims which were not completed, but it is 
      important to evaluate and/or justify why this is the case.
\item Outline any open problems or future plans.  Rather than treat this
      only as an exercise in what you {\em could} have done given more 
      time, try to focus on any unexplored options or interesting outcomes
      (e.g., ``my experiment for X gave counter-intuitive results, this 
      could be because Y and would form an interesting area for further 
      study'').
\end{enumerate}

% =============================================================================

% Finally, after the main matter, the back matter is specified.  This is
% typically populated with just the bibliography.  LaTeX deals with these
% in one of two ways, namely
%
% - inline, which roughly means the author specifies entries using the 
%   \bibitem macro and typesets them manually, or
% - using BiBTeX, which means entries are contained in a separate file
%   (which is essentially a databased) then inported; this is the 
%   approach used below, with the databased being thesis.bib.
%
% Either way, the each entry has a key (or identifier) which can be used
% in the main matter to cite it, e.g., \cite{X}, \cite[Chapter 2}{Y}.

\backmatter

\bibliography{thesis}

% -----------------------------------------------------------------------------

% The thesis concludes with a set of (optional) appendicies; these are the
% same as chapters in a sense, but once signaled as being appendicies via
% the associated macro, LaTeX manages them appropriatly.

\appendix

\chapter{An Example Appendix}
\label{appx:example}

Content which is not central to, but may enhance the thesis can be
included in one or more appendices; examples include, but are not 
limited to

\begin{itemize}
\item lengthy mathematical proofs, numerical or graphical results
      which are summarised in the main body,
\item sample or example calculations, 
      and
\item results of user studies or questionnaires.
\end{itemize}

\noindent
Note that in line with most research conferences, the marking panel 
is not obliged to read such appendices.

% =============================================================================

\end{document}
